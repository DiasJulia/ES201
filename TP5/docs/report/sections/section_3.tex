\section*{Architecture multicoeurs avec des processeurs superscalaires out-of-order (Cortex A15)}

% # 1 thread
% $GEM5/build/ARM/gem5.fast -d results_a15/o3_1t \
%   $GEM5/configs/example/se.py --cpu-type=detailed --caches --l2cache \
%   -n 1 -c ./test_omp -o "1 128"

% # 2 threads
% $GEM5/build/ARM/gem5.fast -d results_a15/o3_2t \
%   $GEM5/configs/example/se.py --cpu-type=detailed --caches --l2cache \
%   -n 2 -c ./test_omp -o "2 128"

% # 4 threads
% $GEM5/build/ARM/gem5.fast -d results_a15/o3_4t \
%   $GEM5/configs/example/se.py --cpu-type=detailed --caches --l2cache \
%   -n 4 -c ./test_omp -o "4 128"

% # 8 threads
% $GEM5/build/ARM/gem5.fast -d results_a15/o3_8t \
%   $GEM5/configs/example/se.py --cpu-type=detailed --caches --l2cache \
%   -n 8 -c ./test_omp -o "8 128"

\subsection*{Q9 : Pour chaque configuration, quel est le nombre de cycles d’exécution de
l’application ? Vous pourrez présenter vos résultats sous forme de graphe 3 axes.}

\textbf{Temps d'exécution simulé (sim\_seconds)} en fonction du nombre de threads et de la largeur du processeur :

% Table with all configurations
\begin{table}[h]
\centering
\caption{Temps d'exécution (sim\_seconds) par largeur de processeur et nombre de threads.}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Width} & \textbf{1 thread} & \textbf{2 threads} & \textbf{4 threads} & \textbf{8 threads} & \textbf{16 threads} \\
\hline
2 voies & 0.000129 & 0.000094 & 0.000077 & 0.000069 & 0.000067 \\
4 voies & 0.000129 & 0.000094 & 0.000077 & 0.000069 & 0.000067 \\
8 voies & 0.000129 & 0.000094 & 0.000077 & 0.000069 & 0.000067 \\
\hline
\end{tabular}
\end{table}

Les résultats montrent que :
\begin{itemize}
    \item Le temps d'exécution diminue avec l'augmentation du nombre de threads (parallelism).
    \item Pour un nombre de threads donné, le temps est \textbf{identique} quel que soit la largeur du processeur (2, 4 ou 8 voies).
    \item Cela suggère que l'application est \textbf{fortement limitée par l'équilibre entre threads} plutôt que par la largeur du processeur superscalaire.
    \item Le temps diminue de 0.000129s (1 thread) à 0.000067s (16 threads), soit une réduction de \textbf{48\%}.
\end{itemize}

\textit{Remarque :} Pour obtenir le \textbf{nombre de cycles}, il manque la valeur \texttt{sim\_ticks} (ou \texttt{numCycles}) et la période d'horloge. Sans ces données, on ne peut pas convertir \texttt{sim\_seconds} en cycles. Cependant, le temps simulé est une métrique valide pour comparer les performances relatives.

\subsection*{Q10 : Déduire le speedup par rapport à la configuration à 1 thread.}

Le \textbf{speedup} est défini comme le rapport du temps de base (1 thread) au temps pour $n$ threads :
\[
\text{Speedup}(n) = \frac{T_1}{T_n}
\]

\begin{table}[h]
\centering
\caption{Speedup par rapport à 1 thread.}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Nombre de threads} & \textbf{2} & \textbf{4} & \textbf{8} & \textbf{16} \\
\hline
Speedup & 1.37 & 1.68 & 1.87 & 1.93 \\
\hline
\end{tabular}
\end{table}

\begin{itemize}
    \item Le speedup est \textbf{sub-linéaire} : il ne croît pas proportionnellement au nombre de threads.
    \item Avec 2 threads, on obtient 1.37× (au lieu du 2× théorique).
    \item Avec 16 threads, on obtient 1.93× (au lieu du 16× théorique).
    \item Cela indique une \textbf{contention sur les ressources partagées} (L2 cache, mémoire, pipeline).
\end{itemize}

\subsection*{Q11 : En utilisant le nombre total d'instructions simulées, déterminez quelle est la valeur
maximale de l'IPC pour chaque configuration ?}

L'\textbf{IPC (Instructions Per Cycle)} mesure le parallélisme au niveau des instructions exécutées par cycle d'horloge.

\begin{table}[h]
\centering
\caption{IPC par largeur du processeur et nombre de threads.}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Width} & \textbf{1 thread} & \textbf{2 threads} & \textbf{4 threads} & \textbf{8 threads} & \textbf{16 threads} \\
\hline
2 voies & 2.128 & N/A & N/A & N/A & N/A \\
4 voies & 2.128 & N/A & N/A & N/A & N/A \\
8 voies & 2.128 & N/A & N/A & N/A & N/A \\
\hline
\end{tabular}
\end{table}

\begin{itemize}
    \item L'IPC pour 1 thread est \textbf{2.128} pour tous les widths (identique).
    \item Les données IPC pour multi-thread ne sont \textbf{pas disponibles} dans les statistiques collectées.
    \item Une IPC de 2.128 indique une bonne exploitation du parallelism au niveau instruction, même avec une largeur de 2.
    \item Le fait que l'IPC soit identique pour 2, 4 et 8 voies suggère que l'application n'est pas limitée par la largeur de l'issue window.
\end{itemize}

\subsection*{Q12 : Discussion et interprétation (max. 10 lignes)}

L'étude des performances du DerivO3CPU avec différents widths et nombres de threads révèle plusieurs points clés :

\textbf{1) Absence d'effet du width :} Les trois configurations (2, 4, 8 voies) donnent des performances \textbf{identiques}. Cela suggère que l'application (multiplication matricielle) ne peut pas exploiter plus de 2 voies d'exécution en parallèle au niveau instruction.

\textbf{2) Parallelism exploité :} Le speedup sub-linéaire (1.93× pour 16 threads) indique une contention sur les ressources partagées, notamment le cache L2 unifié. Les résultats montrent des taux de miss L2 très élevés (0.59 en moyenne), limitant la bande passante mémoire.

\textbf{3) Scalabilité :} L'architecture multicoeur montrait un bon scaling jusqu'à 8 threads, mais avec 16 threads, la contention devient significative, rendant les gains marginaux.