\section*{Architecture multicoeurs avec des processeurs superscalaires out-of-order (Cortex A15)}

% # 1 thread
% $GEM5/build/ARM/gem5.fast -d results_a15/o3_1t \
%   $GEM5/configs/example/se.py --cpu-type=detailed --caches --l2cache \
%   -n 1 -c ./test_omp -o "1 128"

% # 2 threads
% $GEM5/build/ARM/gem5.fast -d results_a15/o3_2t \
%   $GEM5/configs/example/se.py --cpu-type=detailed --caches --l2cache \
%   -n 2 -c ./test_omp -o "2 128"

% # 4 threads
% $GEM5/build/ARM/gem5.fast -d results_a15/o3_4t \
%   $GEM5/configs/example/se.py --cpu-type=detailed --caches --l2cache \
%   -n 4 -c ./test_omp -o "4 128"

% # 8 threads
% $GEM5/build/ARM/gem5.fast -d results_a15/o3_8t \
%   $GEM5/configs/example/se.py --cpu-type=detailed --caches --l2cache \
%   -n 8 -c ./test_omp -o "8 128"

\subsection*{Q9 : Pour chaque configuration, quel est le nombre de cycles d’exécution de
l’application ? Vous pourrez présenter vos résultats sous forme de graphe 3 axes.}

\textbf{Temps d'exécution simulé (sim\_seconds)} en fonction du nombre de threads et de la largeur du processeur :

% Table with all configurations
\begin{table}[h]
\centering
\caption{Temps d'exécution (sim\_seconds) par largeur de processeur et nombre de threads.}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Width} & \textbf{1 thread} & \textbf{2 threads} & \textbf{4 threads} & \textbf{8 threads} & \textbf{16 threads} \\
\hline
2 voies & 0.000169 & 0.000115 & 0.000089 & 0.000075 & 0.000071 \\
4 voies & 0.000116 & 0.000087 & N/A & 0.000066 & 0.000065 \\
8 voies & 0.000114 & 0.000086 & 0.000072 & 0.000066 & N/A \\
\hline
\end{tabular}
\end{table}

Les résultats montrent que :
\begin{itemize}
    \item Le temps d'exécution diminue avec l'augmentation du nombre de threads (parallelism).
    \item Pour un nombre de threads donné, les configurations 4 et 8 voies sont en général plus rapides que 2 voies.
    \item Le meilleur temps observé est \textbf{0.000065 s} (4 voies, 16 threads), contre \textbf{0.000169 s} (2 voies, 1 thread).
    \item Certaines combinaisons sont absentes (\texttt{N/A}) : 4 voies / 4 threads et 8 voies / 16 threads.
\end{itemize}

\textit{Remarque :} Le fichier de résultats inclut aussi \texttt{sim\_ticks}, \texttt{cycles\_max} et \texttt{insts\_max}, ce qui permet d'analyser les tendances en cycles en plus du temps simulé.

\subsection*{Q10 : Déduire le speedup par rapport à la configuration à 1 thread.}

Le \textbf{speedup} est défini comme le rapport du temps de base (1 thread) au temps pour $n$ threads :
\[
\text{Speedup}(n) = \frac{T_1}{T_n}
\]

\begin{table}[h]
\centering
\caption{Speedup par rapport à 1 thread.}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Width} & \textbf{2} & \textbf{4} & \textbf{8} & \textbf{16} \\
\hline
2 voies & 1.47 & 1.90 & 2.25 & 2.38 \\
4 voies & 1.33 & N/A & 1.76 & 1.78 \\
8 voies & 1.33 & 1.58 & 1.73 & N/A \\
\hline
\end{tabular}
\end{table}

\begin{itemize}
    \item Le speedup est \textbf{sub-linéaire} : il ne croît pas proportionnellement au nombre de threads.
    \item En 2 voies, le speedup progresse jusqu'à \textbf{2.38×} à 16 threads.
    \item En 4 voies, on observe \textbf{1.78×} à 16 threads, avec une donnée manquante à 4 threads.
    \item En 8 voies, le speedup atteint \textbf{1.73×} à 8 threads (valeur 16 threads indisponible).
    \item Les écarts entre largeurs et la saturation du gain confirment une contention sur les ressources partagées (cache/mémoire/synchronisation).
\end{itemize}

\subsection*{Q11 : En utilisant le nombre total d'instructions simulées, déterminez quelle est la valeur
maximale de l'IPC pour chaque configuration ?}

L'\textbf{IPC (Instructions Per Cycle)} mesure le parallélisme au niveau des instructions exécutées par cycle d'horloge.

\begin{table}[h]
\centering
\caption{IPC par largeur du processeur et nombre de threads.}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Width} & \textbf{1 thread} & \textbf{2 threads} & \textbf{4 threads} & \textbf{8 threads} & \textbf{16 threads} \\
\hline
2 voies & 1.351054 & 1.800073 & 1.751922 & 1.686834 & 1.578735 \\
4 voies & 1.963032 & 3.199570 & N/A & 2.785343 & 2.659370 \\
8 voies & 2.000776 & 3.379401 & 3.345791 & 3.286575 & N/A \\
\hline
\end{tabular}
\end{table}

\begin{itemize}
    \item L'IPC varie avec la largeur : à 1 thread, il passe de \textbf{1.351} (2 voies) à \textbf{2.001} (8 voies).
    \item En multi-thread, les IPC maximales les plus élevées sont obtenues en 8 voies (jusqu'à \textbf{3.380} à 2 threads).
    \item Les résultats 4 voies et 8 voies sont proches à partir de 8 threads (\textasciitilde2.79 vs \textasciitilde3.29), ce qui suggère une saturation progressive.
    \item Deux combinaisons restent indisponibles dans les mesures (4 voies/4 threads et 8 voies/16 threads).
\end{itemize}

\subsection*{Q12 : Discussion et interprétation (max. 10 lignes)}

L'étude des performances du DerivO3CPU avec différents widths et nombres de threads révèle plusieurs points clés :

\textbf{2) Parallelism limité par la contention :} Le speedup reste sub-linéaire pour toutes les largeurs, ce qui indique des limites côté mémoire/cache/synchronisation malgré l'augmentation du nombre de threads.

\textbf{3) Données partielles :} Deux points expérimentaux manquent (4 voies/4 threads et 8 voies/16 threads). Les tendances globales restent néanmoins cohérentes : gains nets entre 2 et 4/8 voies, puis saturation progressive aux forts niveaux de parallélisme.